{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This..doesn't actually work properly at the moment. A couple issues:\n",
    "* The country name \"Bonaire, Sint Eustatius, and Saba\" needs to be escaped because it contains commas. By default, Pandas does this by wrapping the field in double quotes, but that causes silent breakage in the loaded Hive table in that row. There's probably a way to fix this by altering the escaping settings, but I worked around it by saving it as a Parquet file and then using Spark to load the Parquet file directly to Hive. Since Parquet is a better format than CSV, it doesn't have the escaping problems.\n",
    "* Currently, the `analytics-product` user owns the `canonical_data` database. This means that, to operate on it, you have to have rights to sudo as that user and then you have to run the command as `sudo -u analytics-product kerberos-run-command analytics-product hive -e \"{{SQL}}\"`. This can't be done from inside our Jupyter environment. I worked around this by doing the actual loading into a table in my own database inside Jupyter, then in a plain SSH connection doing the sudo stuff to run the queries `DROP TABLE canonical_data.countries` and `CREATE TABLE canonical_data.countries AS SELECT * FROM nshahquinn.countries`.\n",
    "* (Bonus note: If you load the data from a CSV into Pandas, make sure Namibia's country code \"NA\" is not intepreted as a null.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wmf.hive.load_csv(\n",
    "    \"countries.csv\",\n",
    "    field_spec=\"\"\"\n",
    "        name STRING,\n",
    "        iso_code STRING,\n",
    "        economic_region STRING,\n",
    "        maxmind_continent STRING,\n",
    "        is_protected BOOLEAN\n",
    "    \"\"\",\n",
    "    db_name=\"nshahquinn\",\n",
    "    table_name=\"countries\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
